# ReadingNotes

# AI/NLP前沿知识痛点
**信息过载 + 更新太快**

## 🔑1. 区分 基础不变 vs 应用更新
- 基础不变（长期有效，值得深学）
  - 线性代数、概率统计、优化方法
  - 神经网络基本原理（梯度下降、反向传播、激活函数）
  - Attention 机制、Transformer 架构核心思想（已经稳定下来，不会被完全推翻）
- 应用更新（变化快，选读即可）
  - 各种大模型（GPT-2 → GPT-3 → GPT-4 → Llama → Mistral…）
  - 工程优化（蒸馏、量化、加速库、推理框架）
  - 新的 benchmark、应用 demo
👉 建议：
核心时间放在“不变的基础”上，这样你只要在新论文出来时，对照基础，就能快速看懂新点子。

## 🔑 2. 选择“权威聚合源”而不是到处乱搜
- 英文权威：
  - arXiv Sanity（Yann LeCun 推荐，用于筛选最新论文，按热度排序）
  - Papers with Code（论文 + 开源实现）
  - Lil’Log 博客（OpenAI 研究员写的，图解 + 最新综述）
- 中文聚合：
  - 知乎/公众号（挑一些长期做 NLP 的博主，而不是追热点）
  - Bilibili 上的 李宏毅老师课程（会更新新版，且有大量直观讲解）
👉 方法：别随便搜，找几个固定渠道，每周/每月看一次即可。

## 🔑 3. 建立“版本感知”
AI 领域的论文/博客，常常过两年就有新替代方案。
你可以养成一个习惯：
  - 看资料时，先确认年份
  - 想一想：这是基础原理，还是某一代模型的优化？
例如：
  - Attention (2017 Transformer) → 核心依然有效
  - BERT (2018) → 里程碑，依旧是入门必读
  - GPT 系列 (2019-) → 版本更新快，但本质还是 Transformer
  - FlashAttention (2022) → 工程优化，几年后可能换新
这样你就能避免“花一个月啃 GPT-2 细节，结果 GPT-4 已经变样”的尴尬。
## 🔑 4. 时间管理：用“三层学习圈”法
1. 核心圈（长期学习）
  - 数学 + Transformer 基础，至少啃透 2~3 本书/系统课程
2. 跟进圈（定期更新）
  - 每个月花 2 小时浏览 Papers with Code 热门
  - 每季度看一次综述（Survey），而不是零散论文
3. 应用圈（按需解决）
  - 当你需要某个工具/库时，再去查最新资料
  - 避免陷入“学一堆暂时用不到的优化 trick”
## 🔑 5. 利用 AI 工具帮你“自动更新知识库”
你完全可以把“保持前沿”交给工具：
  - 用 ChatGPT（或者我😎）总结最新论文、对比改进点
  - 用 RSS / 邮件订阅关注 arXiv 的关键词（如 “Transformer”, “LLM”）
  - 用 GitHub Trending 关注 NLP/LLM 热门 repo
这样你不会被动挨打，而是主动“挑选”需要更新的知识。
✨ 总结一句话：
**先打牢“不变的核心”，再用固定渠道获取更新，用工具筛选精华。**这样学习效率最高，也不会因为技术更迭而焦虑。
