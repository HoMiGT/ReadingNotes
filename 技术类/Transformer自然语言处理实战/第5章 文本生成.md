# 第5章 文本生成
## 5.1 生成连贯文本的挑战
通过预训练和监督微调的组合来处理NLP任务。    
模型产生一些logit, 要么取最大值以获得预测类，要么应用softmax函数以获得每个类的预测概率。    

这在文本生成中引入了一些特殊的挑战：
- 解码是迭代进行的，因此涉及的计算量比仅通过模型前向传递的一次传输要大得多。
- 生成的文本质量和多样性取决于解码方法和相关的超参数的选择。

> 步骤1 `Transformers` `are` `the` most    
> 步骤2 `Transformers` `are` `the` `most` popular    
> 步骤3 `Transformers` `are` `the` `most` `popular` toys    

## 5.2 贪婪搜索解码

$$
\hat{y_t}=arg_{y_t}maxP(y_t | y_{< t}, x)
$$

主要缺点：产生重复的输出序列。

## 5.3 束搜索解码
取对数概率是为了避免数字不稳定问题，因为会遇到数字下溢问题。

$$
logP(y_1, ··· , y_t | x) = \sum_{t=1}^{N}logP(y_t | y_{< t}, x)
$$

## 5.4 采样方法
最简单的采用方法是在每个时间步从模型输出的概率分布中随机采样，采样的范围为整个词表：

$$
P(y_t = w_i | y_{< t}, x) = softmax(Z_{t,i}) = \frac{exp(Z_{t,i})}{\sum_{j=1}^{| V |} exp(Z_{t,j})}
$$

其中 |V|表示词表的基数。我们可以通过添加温度参数T来控制输出的多样性，该参数在进行softmax之前重新缩放logit。

$$
P(y_t = w_i | y_{< t}, x) = \frac{exp(z_{t,i} / T)}{\sum_{j=1}^{|V|} exp(Z_{t,j} / T)}
$$

通过调整T,我们可以控制概率分布的形状。 当T $\1eq$ 1 时，分布会在原点附近峰值化，罕见的词元会被压制。 趋势更陡峭            
当T $\geq$ 1时，分布变得平坦，每个词元变得同等可能。 趋势更缓和    

高温导致大部分无异议的语句。    
低温使句子更加连贯。    
控制温度，可以控制样本的质量，在连贯性(低温度)和多样性(高温度)之间总是存在一个权衡，需要根据实际应用场景来调整。

另一种调整连贯性和多样性之间权衡的方法是截断词汇的分布。top-K和核采样

## 5.5 top-K 和 核采样
top-k 采样的思想是通过仅从具有最高概率的k个词元中进行采样来避免低概率选择。这对分布的长尾部分施加了固定的截断，并确保我们只从可能得选择中进行采样。    
top-p 不选择固定的截断值，而是设定一个当达到一定概率质量时的截断条件。    
设置top_k=50和top_p=0.9, 表示从最多50个词元中选择概率质量为90%的词元。    

## 5.6 哪种解码方法最好
模型执行像算术或提供特定问题答案这样的**精确**任务，那么应该**降低温度**或使用**确定性**方法，如将**贪婪搜索**与**束搜索**相结合，以确保获得最可能的答案。    
模型生成更长的文本，甚至有点**创造性**，那么该切换**采样方法**，**增加温度**或使用**top-k**和**核采样**的混合方法。



