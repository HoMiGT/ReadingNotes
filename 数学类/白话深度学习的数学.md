# 第1章 神经网络入门
## 1.2 神经网络所处的位置
AI:
- 基于规则
- 机器学习
  - 朴素贝叶斯
  - SVM
  - 线性回归
  - 感知机
  - 逻辑回归
  - 神经网络
    - DNN
    - CNN
    - RNN
    - GAN

## 1.3 关于神经网络

输入值 -> 输入层 -> 隐藏层 -> 输出层 -> 输出值

圆形表示**神经元**,又被称为**单元**      
单元之间的每个连接都带有叫作**权重**的数值，这些数值是衡量信息的重要性或相关性的指标    

增加层次，网络越来越深而形成的神经网络，叫作**深度神经网络**，又称**DNN**    
学习这种深层的神经网络的权重，叫作**深度学习**，或**深层学习**    
单元之间的连接方式也可以改变。**卷积神经网络**，就是通过更改单元的连接方式而形成的。    

# 1.4 神经网络能做的事情
神经网络实际上就是一个函数 $f(x)$     

回归问题    
分类问题    
生成性的任务    

# 第2章 学习正向传播
## 2.2 感知机的原理

$$
y =
\begin{cases}
0, & (w_{1}x_{1} + w_{2}x_{2} \leq \theta)  \\
1, & (w_{1}x_{1} + w_{2}x_{2} > \theta)
\end{cases}
$$

$$
y=
\begin{cases}
0, & \bigg(\displaystyle\sum_{i=1}^{2} w_{i}x_{i} \leq \theta\bigg) \\
1, & \bigg(\displaystyle\sum_{i=1}^{2} w_{i}x_{i} > \theta\bigg)
\end{cases}
$$


也可以用向量来表示

$$
x = 
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
$$

$$
w = 
\begin{bmatrix}
w_1 \\
w_2
\end{bmatrix}
$$

$$
y = 
\begin{cases}
0, & (w \cdot x \leq \theta)\\
1, & (w \cdot x > \theta)
\end{cases}
$$

## 2.3 感知机和偏置

$$
y = 
\begin{cases}
0, & (w \cdot x + b \leq 0)\\
1, & (w \cdot x + b > 0)
\end{cases}
$$

其中用b表示 $-\theta$ , b叫作**偏置**      

## 2.7 多层感知机

**多层感知机就是神经网络**

**多层构成，且每层的所有单元都由箭头连接起来的网络，叫全连接神经网络**

## 2.9 神经网络的权重

- 权重的数量 = 连接各层中单元的线的数量
- 偏置的数量 = 该层的单元的数量

## 2.10 激活函数

激活函数的核心价值：打破线性限制、提供非线性映射能力、稳定数值范围、优化梯度传播。

没有激活函数，深度网络无法体现深度的优势。

根据阈值输出0或1的函数，叫作阶跃函数。

| 激活函数       | 公式                               | 特点                            |
| ---------- | -------------------------------- | ----------------------------- |
| Sigmoid    | $\sigma(x) = \frac{1}{1+e^{-x}}$ | 输出(0,1)，概率解释，梯度消失问题           |
| Tanh       | $\tanh(x)$                       | 输出(-1,1)，收敛快于Sigmoid，但仍可能梯度消失 |
| ReLU       | $\max(0,x)$                      | 简单高效，稀疏激活，避免梯度消失，但存在“神经元死亡”   |
| Leaky ReLU | $x>0:x,\;x<0:ax$                 | 改进ReLU，缓解神经元死亡问题              |
| Softmax    | $\frac{e^{x_i}}{\sum e^{x_j}}$   | 多分类输出概率分布                     |

## 2.11 神经网络的表达式
两层神经网络的函数表达式：输入层->隐藏层->输出层

$$
f(x^{(0)}) = a^{(2)} \big(W^{(2)}a^{(1)}(W^{(1)}x^{(0)} + b^{(1)})+ b^{(2)})
$$

其中 $a^{(i)}$ 表示对应层的激活函数， $W^{(i)}$ 表示对应层的权重矩阵， $b^{(i)}$ 表示对应层的偏置矩阵 

## 2.12 正向传播
两层神经网络的正向传播：  

$$
\begin{aligned}
x^{(1)} &= a^{(1)}(W^{(1)}x^{(0)} + b^{(1)}) \\         
x^{(2)} &= a^{(2)}(W^{(2)}x^{(1)} + b^{(2)}) \\    
f(x^{(0)}) &= x^{(2)}
\end{aligned}
$$

从左到右一层层传递的操作叫作正向传播

## 2.13 神经网络的通用化

L 层神经网络正向传播的公式：

$$
\begin{aligned}
x^{(1)} &= a^{(1)}(W^{(1)}x^{(0)} + b^{(1)}) \\
x^{(2)} &= a^{(2)}(W^{(2)}x^{(1)} + b^{(2)}) \\
\vdots \\
y &= a^{(L)}(W^{(L)}x^{(L-1)}+b^{(L)})
\end{aligned}
$$

# 第3章 学习反向传播

## 3.4 目标函数
误差公式

$$
\begin{aligned}
E(\Theta) = \frac{1}{2} \sum_{k=1}^{n} (y_k - f(x_k))^2
\end{aligned}
$$

乘以正数，只会是误差之和上下变化，而使误差之和最小的 $\Theta$ 本身的值不会发生变化。    

这种寻找某个函数值最小的参数的问题叫作**最优化问题**    
而在最优化问题中，寻找最小值的函数叫作**目标函数**， $E(\Theta)$ 就是目标函数    

## 3.5 梯度下降法

有表达式为 $g(x) = (x - 1)^2$的函数，求g(x)最小的x。    

微分求解

$$
\begin{aligned}
\frac{dg(x)}{dx} &= \frac{d}{dx}(x-1)^2 \\
    &= \frac{d}{dx}(x^2-2x+1) \\
    &= 2x-2
\end{aligned}
$$

- 当 x < 1 时，增大 x , g(x) 减小
- 当 x > 1 时，减小 x , g(x) 减小

根据导数的符号，创建增减表

|x的范围|$\frac{dg(x)}{dx}$的符号|g(x)的增减|要使g(x)最小，该如何做|
|:--:|:--:|:--:|:--:|
|$x < 1$|-|↘️|增大x|
|$x = 1$|0||已是最小值|
|$x > 1$|+|↗️|减小x|

沿着与导函数的符号相反的方向移动，可直接写成： $x: = x- \frac{dg(x)}{dx}$    

**A:=B** 的意思是 A 是由B定义的。

当x的移动幅度过大时，会导致无法找到最小值，因此需要增加一个参数伊塔，叫做**学习率**，记作 $\eta$ ,此时公式为： 

$$
\begin{aligned}
x := x - \eta\frac{dg(x)}{dx}
\end{aligned}
$$

如果 $\eta$ 太大，会导致x跳来跳去，甚至可能远离最小值，这种状态叫作**发散**。    
反之，如果取较小的 $\eta$ 值，会使x的移动量变小。虽然能够使它接近最小值，但也会增加更新的次数。这种状态叫**收敛**。    

使用微分来解决最优化问题的方法，这种方法叫作**梯度下降法**


更新目标函数的公式：

$$
\begin{aligned}
\Theta := \Theta - \eta\frac{d}{d \Theta}E(\Theta)
\end{aligned}
$$

$\Theta = {W^{(1)},b^{(1)},W^{(2)},b^{(2)}}$

更新参数的**偏微分**：    

$$
\begin{aligned}
w_{ij}^{(l)} := w_{ij}^{(l)} - \eta\frac{\partial E(\Theta)}{\partial w_{ij}^{(l)}}
\end{aligned}
$$

$$
\begin{aligned}
b_{i}^{(l)} := b_{i}^{(l)} - \eta\frac{\partial E(\Theta)}{\partial b_{i}^{(l)}}
\end{aligned}
$$

## 3.6 小技巧：德尔塔
误差之和公式：

$$
\begin{aligned}
E(\Theta) = \frac{1}{2}\sum_{k=1}^{n}(y_{k} - f(x_k))^2
\end{aligned}
$$

拆解开，一项项的求，如：

$$
\begin{aligned}
E_{k}(\Theta) = \frac{1}{2}(y_k - f(x_k))^2
\end{aligned}
$$

总结就是：先求误差之和，再计算整体的偏微分，和先计算各个误差的偏微分，再求和，结果是一样的。即如下式：

$$
\begin{aligned}
\frac{\partial}{\partial w_{ij}^{(l)}}\big(\sum_{k=1}^{n}E_{k}(\Theta)\big) = \sum_{k=1}^{n}\big(\frac{\partial E_{k}(\Theta)}{\Theta w_{11}^{(l)}}\big)
\end{aligned}
$$


例如在处理有k个数据的 $x_k$ 时，神经网络层与层之间传递数值计算过程如下所示：

$$
\begin{aligned}
&x^{(0)} = x_k    \cdots\ \text{输出层} \\
&x^{(1)} = a^{(1)} \big(W^{(1)}x^{(0)} + b^{(1)} \big)    \cdots\ \text{第 1 层} \\
&x^{(2)} = a^{(2)} \big(W^{(2)}x^{(1)} + b^{(2)} \big)    \cdots\ \text{第 2 层} \\
&x^{(3)} = a^{(3)} \big(W^{(3)}x^{(2)} + b^{(3)} \big)    \cdots\ \text{第 3 层} \\
&f(x_k) = x^{(3)}
\end{aligned}
$$

反过来从输出值的角度来看:    

$$\displaystyle
\begin{aligned}
f(x_k) &= x^{(3)} \quad \cdots\ \text{输出层} \\
    &= a^{(3)} \big(W^{(3)}x^{(2)} + b^{(3)} \big) \quad \cdots\ \text{第 3 层} \\
    &= a^{(3)} \Bigg(
        \begin{bmatrix}
        w_{11}^{(3)} & w_{12}^{(3)}
        \end{bmatrix}
        \begin{bmatrix}
        x_{1}^{(2)} \\
        x_{2}^{(2)}
        \end{bmatrix}
        +
        \begin{bmatrix}
        b_{1}^{(3)}
        \end{bmatrix}
        \Bigg) \\
    &= a^{(3)}(w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)}) \quad \cdots\ \text{展开权重和偏置}
\end{aligned}
$$

令 $z_{1}^{(3)} = w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)}$ ,可以把z1看作是应用激活函数之前的第3层的第1个单元的输入，也叫**加权输入**     

那么误差可以写成如下公式：

$$
\begin{aligned}
E_{k}(\Theta) &= \frac{1}{2}(y_k - f(x_k))^2 \\
    &= \frac{1}{2}(y_k - a^{(3)}z_{1}^{(3)})^2 
\end{aligned}
$$

那么微分方程就为：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{11}^{(3)}} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \cdot \frac{\partial z_{1}^{(3)}}{\partial w_{11}^{(3)}}
\end{aligned}
$$

那么 $z_{1}^{(3)}$ 对 $w_{11}^{(3)}$ 的偏微分，如下式：

$$
\begin{aligned}
\frac{\partial z_{1}^{(3)}}{\partial w_{11}^{(3)}} &= \frac{\partial}{\partial w_{11}^{(3)}} \big( w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)} \big) \\
    &= x_{1}^{(2)}
\end{aligned}
$$

接下来，用德尔塔来代替，如下所示：

$$
\delta_{1}^{(3)} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}}
$$

$\delta$ **常用来表示微小的变化量**。

那么

$$
\begin{aligned}
& \frac{\partial E_{k}(\Theta)}{\partial w_{11}^{(3)}} = \delta_{1}^{(3)} \cdot x_{1}^{(2)} \\
& \frac{\partial E_{k}(\Theta)}{\partial w_{12}^{(3)}} = \delta_{1}^{(3)} \cdot x_{2}^{(2)} 
\end{aligned}
$$

推广成一般，表达式变为如下格式：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}} = \delta_{i}^{(l)} \cdot x_{j}^{(l)} \quad \big( \delta_{i}^{(l)} = \frac{\partial E_{k}(\Theta)}{\partial z_{i}^{(l)}} \big)
\end{aligned}
$$

**理念：德尔塔的复用**

## 3.7 德尔塔的计算
1. 输出层的计算

$$
\begin{aligned}
\text{设} v &= y_k - a^{(3)} \cdot z_{1}^{(3)} \\
E_{k}(\Theta) &= \frac{1}{2}v^2
\end{aligned}
$$

$z_{1}^{(3)}$ 包含在 v 中，v 包含在 $E_{k}(\Theta)$    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} &= \frac{\partial{E_{k}(\Theta)}}{\partial v} \cdot \frac{\partial v}{\partial z_{1}^{(3)}} \\
\frac{\partial{E_{k}(\Theta)}}{\partial v} &= \frac{\partial}{\partial v}\big(\frac{1}{2}v^2 \big) \\
    &= v \\
\frac{\partial v}{\partial z_{1}^{(3)}} &= \frac{\partial}{\partial z_{1}^{(3)}}\big(y_k - a^{(3)} \cdot z_{1}^{(3)} \big) \\
    &= -a \prime ^{3}(z_{1}^{3}) \quad \cdots  -a \prime ^{3}(z_{1}^{3}) == \frac{\partial a^{(3)}(z_{1}^{(3)})}{\partial z_{1}^{(3)}}
\end{aligned}
$$

综上：

$$
\begin{aligned}
\delta_{1}^{(3)} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \\
    &= \frac{\partial E_{k}(\Theta)}{\partial v} \cdot \frac{\partial v}{\partial z_{1}^{(3)}} \\
    &= v \cdot a \prime ^{(3)}(z_{1}^{(3)}) \\
    &= (y_k - a^{(3)}(z_{1}^{(3)})) \cdot - a \prime ^{(3)}(z_{1}^{(3)}) \\
    &= (a^{(3)}(z_{1}^{(3)}) - y_k) \cdot + a \prime ^{(3)}(z_{1}^{(3)})
\end{aligned}
$$

如果激活函数是Sigmoid函数，则微分可表示成：

$$
\begin{aligned}
a \prime ^{(3)}(z_{1}^{(3)}) = (1 - a^{(3)}(z_{1}^{(3)})) \cdot a^{(3)}(z_{1}^{(3)})
\end{aligned}
$$



