# 第1章 神经网络入门
## 1.2 神经网络所处的位置
AI:
- 基于规则
- 机器学习
  - 朴素贝叶斯
  - SVM
  - 线性回归
  - 感知机
  - 逻辑回归
  - 神经网络
    - DNN
    - CNN
    - RNN
    - GAN

## 1.3 关于神经网络

输入值 -> 输入层 -> 隐藏层 -> 输出层 -> 输出值

圆形表示**神经元**,又被称为**单元**      
单元之间的每个连接都带有叫作**权重**的数值，这些数值是衡量信息的重要性或相关性的指标    

增加层次，网络越来越深而形成的神经网络，叫作**深度神经网络**，又称**DNN**    
学习这种深层的神经网络的权重，叫作**深度学习**，或**深层学习**    
单元之间的连接方式也可以改变。**卷积神经网络**，就是通过更改单元的连接方式而形成的。    

# 1.4 神经网络能做的事情
神经网络实际上就是一个函数 $f(x)$     

回归问题    
分类问题    
生成性的任务    

# 第2章 学习正向传播
## 2.2 感知机的原理

$$
y =
\begin{cases}
0, & (w_{1}x_{1} + w_{2}x_{2} \leq \theta)  \\
1, & (w_{1}x_{1} + w_{2}x_{2} > \theta)
\end{cases}
$$

$$
y=
\begin{cases}
0, & \bigg(\displaystyle\sum_{i=1}^{2} w_{i}x_{i} \leq \theta\bigg) \\
1, & \bigg(\displaystyle\sum_{i=1}^{2} w_{i}x_{i} > \theta\bigg)
\end{cases}
$$


也可以用向量来表示

$$
x = 
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
$$

$$
w = 
\begin{bmatrix}
w_1 \\
w_2
\end{bmatrix}
$$

$$
y = 
\begin{cases}
0, & (w \cdot x \leq \theta)\\
1, & (w \cdot x > \theta)
\end{cases}
$$

## 2.3 感知机和偏置

$$
y = 
\begin{cases}
0, & (w \cdot x + b \leq 0)\\
1, & (w \cdot x + b > 0)
\end{cases}
$$

其中用b表示 $-\theta$ , b叫作**偏置**      

## 2.7 多层感知机

**多层感知机就是神经网络**

**多层构成，且每层的所有单元都由箭头连接起来的网络，叫全连接神经网络**

## 2.9 神经网络的权重

- 权重的数量 = 连接各层中单元的线的数量
- 偏置的数量 = 该层的单元的数量

## 2.10 激活函数

激活函数的核心价值：打破线性限制、提供非线性映射能力、稳定数值范围、优化梯度传播。

没有激活函数，深度网络无法体现深度的优势。

根据阈值输出0或1的函数，叫作阶跃函数。

| 激活函数       | 公式                               | 特点                            |
| ---------- | -------------------------------- | ----------------------------- |
| Sigmoid    | $\sigma(x) = \frac{1}{1+e^{-x}}$ | 输出(0,1)，概率解释，梯度消失问题           |
| Tanh       | $\tanh(x)$                       | 输出(-1,1)，收敛快于Sigmoid，但仍可能梯度消失 |
| ReLU       | $\max(0,x)$                      | 简单高效，稀疏激活，避免梯度消失，但存在“神经元死亡”   |
| Leaky ReLU | $x>0:x,\;x<0:ax$                 | 改进ReLU，缓解神经元死亡问题              |
| Softmax    | $\frac{e^{x_i}}{\sum e^{x_j}}$   | 多分类输出概率分布                     |

## 2.11 神经网络的表达式
两层神经网络的函数表达式：输入层->隐藏层->输出层

$$
f(x^{(0)}) = a^{(2)} \big(W^{(2)}a^{(1)}(W^{(1)}x^{(0)} + b^{(1)})+ b^{(2)})
$$

其中 $a^{(i)}$ 表示对应层的激活函数， $W^{(i)}$ 表示对应层的权重矩阵， $b^{(i)}$ 表示对应层的偏置矩阵 

## 2.12 正向传播
两层神经网络的正向传播：  

$$
\begin{aligned}
x^{(1)} &= a^{(1)}(W^{(1)}x^{(0)} + b^{(1)}) \\         
x^{(2)} &= a^{(2)}(W^{(2)}x^{(1)} + b^{(2)}) \\    
f(x^{(0)}) &= x^{(2)}
\end{aligned}
$$

从左到右一层层传递的操作叫作正向传播

## 2.13 神经网络的通用化

L 层神经网络正向传播的公式：

$$
\begin{aligned}
x^{(1)} &= a^{(1)}(W^{(1)}x^{(0)} + b^{(1)}) \\
x^{(2)} &= a^{(2)}(W^{(2)}x^{(1)} + b^{(2)}) \\
\vdots \\
y &= a^{(L)}(W^{(L)}x^{(L-1)}+b^{(L)})
\end{aligned}
$$

# 第3章 学习反向传播

## 3.4 目标函数
误差公式

$$
\begin{aligned}
E(\Theta) = \frac{1}{2} \sum_{k=1}^{n} (y_k - f(x_k))^2
\end{aligned}
$$

乘以正数，只会是误差之和上下变化，而使误差之和最小的 $\Theta$ 本身的值不会发生变化。    

这种寻找某个函数值最小的参数的问题叫作**最优化问题**    
而在最优化问题中，寻找最小值的函数叫作**目标函数**， $E(\Theta)$ 就是目标函数    

## 3.5 梯度下降法

有表达式为 $g(x) = (x - 1)^2$的函数，求g(x)最小的x。    

微分求解

$$
\begin{aligned}
\frac{dg(x)}{dx} &= \frac{d}{dx}(x-1)^2 \\
    &= \frac{d}{dx}(x^2-2x+1) \\
    &= 2x-2
\end{aligned}
$$

- 当 x < 1 时，增大 x , g(x) 减小
- 当 x > 1 时，减小 x , g(x) 减小

根据导数的符号，创建增减表

|x的范围|$\frac{dg(x)}{dx}$的符号|g(x)的增减|要使g(x)最小，该如何做|
|:--:|:--:|:--:|:--:|
|$x < 1$|-|↘️|增大x|
|$x = 1$|0||已是最小值|
|$x > 1$|+|↗️|减小x|

沿着与导函数的符号相反的方向移动，可直接写成： $x: = x- \frac{dg(x)}{dx}$    

**A:=B** 的意思是 A 是由B定义的。

当x的移动幅度过大时，会导致无法找到最小值，因此需要增加一个参数伊塔，叫做**学习率**，记作 $\eta$ ,此时公式为： 

$$
\begin{aligned}
x := x - \eta\frac{dg(x)}{dx}
\end{aligned}
$$

如果 $\eta$ 太大，会导致x跳来跳去，甚至可能远离最小值，这种状态叫作**发散**。    
反之，如果取较小的 $\eta$ 值，会使x的移动量变小。虽然能够使它接近最小值，但也会增加更新的次数。这种状态叫**收敛**。    

使用微分来解决最优化问题的方法，这种方法叫作**梯度下降法**


更新目标函数的公式：

$$
\begin{aligned}
\Theta := \Theta - \eta\frac{d}{d \Theta}E(\Theta)
\end{aligned}
$$

$\Theta = {W^{(1)},b^{(1)},W^{(2)},b^{(2)}}$

更新参数的**偏微分**：    

$$
\begin{aligned}
w_{ij}^{(l)} := w_{ij}^{(l)} - \eta\frac{\partial E(\Theta)}{\partial w_{ij}^{(l)}}
\end{aligned}
$$

$$
\begin{aligned}
b_{i}^{(l)} := b_{i}^{(l)} - \eta\frac{\partial E(\Theta)}{\partial b_{i}^{(l)}}
\end{aligned}
$$

## 3.6 小技巧：德尔塔
误差之和公式：

$$
\begin{aligned}
E(\Theta) = \frac{1}{2}\sum_{k=1}^{n}(y_{k} - f(x_k))^2
\end{aligned}
$$

拆解开，一项项的求，如：

$$
\begin{aligned}
E_{k}(\Theta) = \frac{1}{2}(y_k - f(x_k))^2
\end{aligned}
$$

总结就是：先求误差之和，再计算整体的偏微分，和先计算各个误差的偏微分，再求和，结果是一样的。即如下式：

$$
\begin{aligned}
\frac{\partial}{\partial w_{ij}^{(l)}}\big(\sum_{k=1}^{n}E_{k}(\Theta)\big) = \sum_{k=1}^{n}\big(\frac{\partial E_{k}(\Theta)}{\Theta w_{11}^{(l)}}\big)
\end{aligned}
$$


例如在处理有k个数据的 $x_k$ 时，神经网络层与层之间传递数值计算过程如下所示：

$$
\begin{aligned}
&x^{(0)} = x_k    \cdots\ \text{输出层} \\
&x^{(1)} = a^{(1)} \big(W^{(1)}x^{(0)} + b^{(1)} \big)    \cdots\ \text{第 1 层} \\
&x^{(2)} = a^{(2)} \big(W^{(2)}x^{(1)} + b^{(2)} \big)    \cdots\ \text{第 2 层} \\
&x^{(3)} = a^{(3)} \big(W^{(3)}x^{(2)} + b^{(3)} \big)    \cdots\ \text{第 3 层} \\
&f(x_k) = x^{(3)}
\end{aligned}
$$

反过来从输出值的角度来看:    

$$\displaystyle
\begin{aligned}
f(x_k) &= x^{(3)} \quad \cdots\ \text{输出层} \\
    &= a^{(3)} \big(W^{(3)}x^{(2)} + b^{(3)} \big) \quad \cdots\ \text{第 3 层} \\
    &= a^{(3)} \Bigg(
        \begin{bmatrix}
        w_{11}^{(3)} & w_{12}^{(3)}
        \end{bmatrix}
        \begin{bmatrix}
        x_{1}^{(2)} \\
        x_{2}^{(2)}
        \end{bmatrix}
        +
        \begin{bmatrix}
        b_{1}^{(3)}
        \end{bmatrix}
        \Bigg) \\
    &= a^{(3)}(w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)}) \quad \cdots\ \text{展开权重和偏置}
\end{aligned}
$$

令 $z_{1}^{(3)} = w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)}$ ,可以把z1看作是应用激活函数之前的第3层的第1个单元的输入，也叫**加权输入**     

那么误差可以写成如下公式：

$$
\begin{aligned}
E_{k}(\Theta) &= \frac{1}{2}(y_k - f(x_k))^2 \\
    &= \frac{1}{2}(y_k - a^{(3)}z_{1}^{(3)})^2 
\end{aligned}
$$

那么微分方程就为：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{11}^{(3)}} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \cdot \frac{\partial z_{1}^{(3)}}{\partial w_{11}^{(3)}}
\end{aligned}
$$

那么 $z_{1}^{(3)}$ 对 $w_{11}^{(3)}$ 的偏微分，如下式：

$$
\begin{aligned}
\frac{\partial z_{1}^{(3)}}{\partial w_{11}^{(3)}} &= \frac{\partial}{\partial w_{11}^{(3)}} \big( w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)} \big) \\
    &= x_{1}^{(2)}
\end{aligned}
$$

接下来，用德尔塔来代替，如下所示：

$$
\delta_{1}^{(3)} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}}
$$

$\delta$ **常用来表示微小的变化量**。

那么

$$
\begin{aligned}
& \frac{\partial E_{k}(\Theta)}{\partial w_{11}^{(3)}} = \delta_{1}^{(3)} \cdot x_{1}^{(2)} \\
& \frac{\partial E_{k}(\Theta)}{\partial w_{12}^{(3)}} = \delta_{1}^{(3)} \cdot x_{2}^{(2)} 
\end{aligned}
$$

推广成一般，表达式变为如下格式：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}} = \delta_{i}^{(l)} \cdot x_{j}^{(l)} \quad \big( \delta_{i}^{(l)} = \frac{\partial E_{k}(\Theta)}{\partial z_{i}^{(l)}} \big)
\end{aligned}
$$

**理念：德尔塔的复用**

## 3.7 德尔塔的计算
1. 输出层的计算

$$
\begin{aligned}
\text{设} v &= y_k - a^{(3)} \cdot z_{1}^{(3)} \\
E_{k}(\Theta) &= \frac{1}{2}v^2
\end{aligned}
$$

$z_{1}^{(3)}$ 包含在 v 中，v 包含在 $E_{k}(\Theta)$    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} &= \frac{\partial{E_{k}(\Theta)}}{\partial v} \cdot \frac{\partial v}{\partial z_{1}^{(3)}} \\
\frac{\partial{E_{k}(\Theta)}}{\partial v} &= \frac{\partial}{\partial v}\big(\frac{1}{2}v^2 \big) \\
    &= v \\
\frac{\partial v}{\partial z_{1}^{(3)}} &= \frac{\partial}{\partial z_{1}^{(3)}}\big(y_k - a^{(3)} \cdot z_{1}^{(3)} \big) \\
    &= -a \prime ^{3}(z_{1}^{3}) \quad \cdots  -a \prime ^{3}(z_{1}^{3}) == \frac{\partial a^{(3)}(z_{1}^{(3)})}{\partial z_{1}^{(3)}}
\end{aligned}
$$

综上：

$$
\begin{aligned}
\delta_{1}^{(3)} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \\
    &= \frac{\partial E_{k}(\Theta)}{\partial v} \cdot \frac{\partial v}{\partial z_{1}^{(3)}} \\
    &= v \cdot a \prime ^{(3)}(z_{1}^{(3)}) \\
    &= (y_k - a^{(3)}(z_{1}^{(3)})) \cdot - a \prime ^{(3)}(z_{1}^{(3)}) \\
    &= (a^{(3)}(z_{1}^{(3)}) - y_k) \cdot + a \prime ^{(3)}(z_{1}^{(3)})
\end{aligned}
$$

如果激活函数是Sigmoid函数，则微分可表示成：

$$
\begin{aligned}
a \prime ^{(3)}(z_{1}^{(3)}) = (1 - a^{(3)}(z_{1}^{(3)})) \cdot a^{(3)}(z_{1}^{(3)})
\end{aligned}
$$

2. 隐藏层的计算

$$
\begin{aligned}
& \delta_{1}^{(2)} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(2)}} ,  \delta_{2}^{(2)} = \frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(2)}} \quad \cdots \cdots \text{第 2 层的德尔塔} \\
& \delta_{1}^{(1)} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} ,  \delta_{2}^{(1)} = \frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(1)}} \quad \cdots \cdots \text{第 1 层的德尔塔}
\end{aligned}
$$


- 输入层(0层):  $x_1$ , $x_2$
- 隐藏层(1层):  $z_{1}^{(1)} \quad \delta_{1}^{(1)}$ , $z_{2}^{(1)} \quad \delta_{2}^{(1)}$
- 隐藏层(2层):  $z_{1}^{(2)} \quad \delta_{1}^{(2)}$ , $z_{2}^{(2)} \quad \delta_{2}^{(2)}$
- 隐藏层(3层):  $z_{1}^{(3)} \quad \delta_{1}^{(3)}$
- 输出层: y

$z_{1}^{(2)}$ 在 $z_{1}^{(3)}$ 中， $z_{1}^{(3)}$ 在 $E_{k}(\Theta)$ 中    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}(2)} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \cdot \frac{\partial z_{1}^{(3)}}{\partial z_{1}^{(2)}} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}(2)} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \cdot \frac{\partial z_{1}^{(3)}}{\partial z_{2}^{(2)}}
\end{aligned}
$$

同理 $z_{1}^{(1)}$ 包含在 $z_{1}^{(2)}$ 和 $z_{2}^{(2)}$ 中， 而 $z_{1}^{(2)}$ 和 $z_{2}^{(2)}$ 包含在 $E_{k}(\Theta)$ 中    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(2)}} \cdot \frac{\partial z_{1}^{(2)}}{\partial z_{1}^{(1)}} + 
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(2)}} \cdot \frac{\partial z_{2}^{(2)}}{\partial z_{1}^{(1)}}
\end{aligned}
$$

求和符号整理成：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{1}^{(1)}} \big) \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{2}^{(1)}} \big)
\end{aligned}
$$

综上各层的公式如下：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(2)}} &= \sum_{r=1}^{1} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(3)}} \cdot \frac{\partial z_{r}^{(3)}}{\partial z_{1}^{(2)}} \big) \quad \cdots \cdots \text{第 2 层第 1 个德尔塔} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(2)}} &= \sum_{r=1}^{1} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(3)}} \cdot \frac{\partial z_{r}^{(3)}}{\partial z_{2}^{(2)}} \big) \quad \cdots \cdots \text{第 2 层第 2 个德尔塔} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{1}^{(1)}} \big) \quad \cdots \cdots \text{第 1 层第 1 个德尔塔} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{2}^{(1)}} \big) \quad \cdots \cdots \text{第 1 层第 2 个德尔塔}
\end{aligned}
$$

总结：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{i}^{(l)}} = \sum_{r=1}^{m^{(l+1)}} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(l+1)}} \cdot \frac{\partial z_{r}^{(l+1)}}{\partial z_{i}^{(l)}} \big)
\end{aligned}
$$

而 $z_{r}^{(l+1)}$ 展开

$$
\begin{aligned}
z_{r}^{(l+1)} &= w_{r1}^{(l+1)}x_{1}^{(l)} + \cdots + w_{ri}^{(l+1)}x_{i}^{(l)} + \cdots \\
    &= w_{r1}^{(l+1)}a^{(l)}(z_{1}^{(l)}) + \cdots + w_{ri}^{(l+1)}a^{(l)}(z_{i}^{(l)}) + \cdots
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial z_{r}^{(l+1)}}{\partial z_{i}^{(l)}} = w_{ri}^{(l+1)} a \prime ^{(l)}(z_{i}^{(l)})
\end{aligned}
$$

目标函数 $E_{k}(\Theta)$ 对加权输入的偏微分，正式德尔塔    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(l+1)}} = \delta_{r}^{(l+1)}
\end{aligned}
$$

综上隐藏层的德尔塔的表达式如下：

$$
\begin{aligned}
\delta_{i}^{(l)} = \sum_{r=1}^{m^{(l+1)}} \big( \delta_{r}^{(l+1)} \cdot w_{ri}^{(l+1)} a \prime ^{(l)}(z_{i}^{(l)}) \big)
\end{aligned}
$$

## 3.8 方向传播

$$
\begin{aligned}
\delta_{i}^{(L)} &= \big( a^{(L)}(z_{i}^{(L)}) - y_k \big) \cdot a \prime ^{(L)}(z_{i}^{(L)})  \quad \cdots\cdots \text{输出层的德尔塔} \\
\delta_{i}^{(l)} &= \sum_{r=1}^{m^{(l+1)}} \big( \delta_{r}^{(l+1)} \cdot w_{ri}^{(l+1)} a \prime ^{(l)}(z_{i}^{(l)}) \big) \quad \cdots\cdots \text{隐藏层的德尔塔}
\end{aligned}
$$

从后面依次计算德尔塔，就能复用已经计算好的德尔塔
- 首先求第3层(输出层)的德尔塔
- 求第2层的德尔塔时，复用紧随其后的第3层的德尔塔
- 求第1层的德尔塔时，复用紧随其后的第2层的德尔塔

为了使用这个更新表达式更新权重，需要计算目标函数 $E_{k}(\Theta)$ 对权重的偏微分 

$$
\begin{aligned}
w_{ij}^{(l)} := w_{ij}^{(l)} - \eta \frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}}
\end{aligned}
$$

直接计算偏微分非常麻烦，所以使用德尔塔来间接地计算偏微分

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}} = \delta_{i}^{(l)} - x_{j}^{(l-1)}
\end{aligned}
$$

将所有表达式整合

$$
\begin{aligned}
\delta_{i}^{(L)} &= \big( a^{(L)}(z_{i}^{(L)}) - y_k \big) a \prime ^{(L)}(z_{i}^{(L)}) \quad \cdots\cdots \text{输出层的德尔塔}  \\ 
\delta_{i}^{(l)} &= a \prime ^{(l)}(z_{i}^{(l)}) \sum_{r=1}^{m^{(l+1)}} \delta_{r}^{(l+1)} w_{ri}^{(l+1)} \quad \cdots\cdots \text{隐藏层的德尔塔} \\
w_{ij}^{(l)} := w_{ij}^{(l)} - \eta \frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}} \quad \cdots\cdots \text{权重的更新表达式} \\
\end{aligned}
$$

从后面的层开始计算德尔塔，以更新权重和偏置的方法叫作误差反向传播法，backpropagation ,又称 backward

# 第 4 章 学习卷积神经网络

## 4.2 卷积过滤器

过滤器的数组叫作核 

在卷积神经网络中，我们把之前称之为过滤器的矩阵叫作**卷积过滤器**或**卷积矩阵**，而将它应用于图像的操作叫作**卷积**    

过滤器又可称为特征检测器

 
$$\left[
\begin{matrix}
0.11 & 0.11 & 0.11 \\
0.11 & 0.11 & 0.11 \\
0.11 & 0.11 & 0.11
\end{matrix}
\right]
\tag{模糊矩阵}
$$

$$\left[
\begin{matrix}
0 & 1 & 0 \\
0 & -1 & 0 \\
0 & 0 & 0
\end{matrix}
\right]
\tag{纵向边缘矩阵}
$$

$$\left[
\begin{matrix}
0 & 0 & 0 \\
0 & -1 & 1 \\
0 & 0 & 0
\end{matrix}
\right]
\tag{横向边缘矩阵}
$$

$$\left[
\begin{matrix}
0 & 1 & 0 \\
1 & -4 & 1 \\
0 & 1 & 0
\end{matrix}
\right]
\tag{全边缘矩阵}
$$

## 4.3 特征图
在卷积神经网络的语境下，应用了卷积过滤器后的图像叫作特征图  

如果想调整特征图的大小，可以使用
- 填充：根据过滤器的大小，在输入图像的外框填充一些数字，以增加输入图像的大小。一般使用0作为填充的数字。
- 步进：指过滤器的移动幅度。

## 4.4 激活函数

ReLU: a(x) = max(0,x) 
Leaky ReLU: 

$$
LeakyReLU(x) =
\begin{cases}
x, & \big( x > 0 \big) \\
\alpha x, & \big( x \leq 0 \big)  \quad \text{(通常} \alpha \text{的值为0.01，用来调整负值的零梯度问题)}
\end{cases}
$$

## 4.5 池化
池化时用于减小特征图大小的处理。      
最常见的池化处理是抽出特定范围内的最大值，这叫做**最大池化**

这种池化处理能使从特征图中提取的特征不容易受到图像变形和移动等的影响。

## 4.6 卷积层

1. 输入图像
2. 第 1 组层： 卷积过滤器 -> 特征图 -> ReLU和池化
3. 第 2 组层： 卷积过滤器 -> 特征图 -> ReLU和池化

$\vdots$

n. 第 n-1 组层： 卷积过滤器 -> 特征图 -> ReLU和池化
n+1. 输出

## 4.7 卷积层的正向传播

x: 输入图像
w: 相当于权重的过滤器
z: 应用激活函数之前的特征图

输入图像由通道、纵、横这3个维度组成，用 $x_{(c,i,j)}$ 来表示

卷积过滤器，除了通道、纵、横，用k来表示第几个过v过滤器，用 $w_{(c,u,v)}^{k}$ 来表示

卷积神经网络的偏置是为各个卷积过滤器定义的，用 $b_{(k)}$ 来表示

特征图，用 $z_{(i,j)}^{(k)}$ 来表示

|字母|定义|上下标|
|:--:|:--:|:--:|
|$x_{(c,i,j)}$|输入图像|c=通道，i=纵，j=横|
|$w_{(c,u,v)}^{k}$|卷积过滤器|k=过滤器编号<br/>c=通道，u=纵，v=横|
|$b_{(k)}$|偏置|k=过滤器编号|
|$z_{(i,j)}^{(k)}$|特征图|k=过滤器编号=特征图的通道<br/>i=纵，j=横|

特征图公式：

$$
\begin{aligned}
z_{(i,j)}^{k} = \sum_{c=1}^{C} \sum_{u=1}^{m} \sum_{v=1}^{m} w_{(c,u,v)}^{(k)}x_{(c,i+u-1,j+v-1)} + b^{(k)}
\end{aligned}
$$

产生特征图，应用激活函数：

$$
a_{(i,j)}^{(k)} = max \big(0, z_{(i,j)}^{(k)} \big)
$$

然后是最大池化，用p来表示池化处理选中的值

$$
\begin{aligned}
p_{(i,j)}^{(k)} = max \big( &a_{2(i-1)+1, 2(j-1)+1}^{(k)} \quad \cdots\cdots \text{左上格}  \\
    &, a_{2(i-1)+2, 2(j-1)+1}^{(k)} \quad \cdots\cdots \text{左上格} \\
    &, a_{2(i-1)+1, 2(j-1)+2}^{(k)} \quad \cdots\cdots \text{右上格} \\
    &, a_{2(i-1)+2, 2(j-1)+2}^{(k)} \quad \cdots\cdots \text{右下格} \\
\big)
\end{aligned}
$$

简写

$$
\begin{aligned}
p_{(i,j)}^{(k)} = P_{m_{p}} \big( a_{(i,j)}^{(k)} \big)
\end{aligned}
$$

至此，卷积层的公式如下：

$$
\begin{aligned}
z_{(i,j)}^{(k)} &= \sum_{c=1}^{C}\sum_{u=1}^{m}\sum_{v=1}^{m}w_{(c,u,v)}^{(k)}x_{(c,i+u-1,j+v-1)} + b^{k} \quad \cdots\cdots\ \text{卷积} \\
a_{(i,j)}^{(k)} &= max\big(0,z_{(i,j)}^{(k)}\big) \quad \cdots\cdots \text{激活函数} \\
p_{(i,j)}^{(k)} &= P_{m_{p}} \big( a_{(i,j)}^{(k)} \big) \quad \cdots\cdots \text{池化}
\end{aligned}
$$

最后一个输出是下一层卷积的输入

$$
\begin{aligned}
x_{(c,i,j)}^{(k+1)} = p_{(i,j)}^{(k)}
\end{aligned}
$$

## 4.8 全连接层的正向传播

在分类问题中，会使用一个softmax的函数作为输出层的激活函数    
设输出层的加权输入是  $z^{(4)} = W^{(4)}x^{(3)} + b^{(4)}$ 其中第i个加权输入是 $z_{i}^{(4)}$ 对其应用softmax函数的表达式：

$$
\begin{aligned}
a^{(4)}\big(z_{i}^{(4)} = \frac{exp(z_{i}^{(4)})}{\sum_{j}exp(z_{j}^{(4)})}
\end{aligned}
$$

**大部分分类问题，最后都会使用softmax函数**

## 4.9 反向传播
误差定义：**交叉熵**    
交叉熵：是衡量俩个概率分布 P(w) 和 Q(w) 之间的相似度的值，当俩个分布相同，也就是 P(w) = Q(w)时，交叉熵的值最小。      

目标函数：

$$
\begin{aligned}
E(\Theta) &= \sum_{p=1}^{n}t_p\cdot log_{e}\frac{1}{y_p} \\
  &= -\sum_{p=1}{n}t_p\cdot log_{e}y_p
\end{aligned}
$$

可以认为 $\Theta$ 包含了卷积滤波器的权重、偏置，以及全连接层的权重、偏置等全部包含在内。      

$$
\begin{aligned}
w_{(ij)}^{(l)} &:= w_{(ij)}^{(l)} - \eta\frac{\partial E(\Theta)}{\partial w_{ij}^{(l)}} \quad \cdots\cdots \text{全连接层的权重} \\
b_{i}^{(l)} &:= b_{i}^{(l)} -\eta\frac{\partial E(\Theta)}{\partial b_{i}^{(l)}} \quad \cdots\cdots \text{全连接层的偏置} \\
w_{(u,v,c)}^{(k,l)} &:= w_{(u,v,c)}^{(k,l)} - \eta\frac{\partial E(\Theta)}{\partial w_{(u,v,c)}^{(k,l)}} \quad \cdots\cdots \text{卷积过滤器的权重} \\
b^{(k,l)} := b^{(k,l)} - \eta\frac{\partial E(\Theta)}{\partial b^{(k,l)}} \quad \cdots\cdots \text{卷积过滤器的偏置}
\end{aligned}
$$

全连接层的更新表达式  $\delta_{i}^{(4)} = -t_i+y_i$

卷积过滤器的更新表达式 

$$
\begin{aligned}
\frac{\partial E(\Theta)}{\partial w_{(c,u,v)}} = \sum_{i=1}^{d}\sum_{j=1}^{d}\delta_{(i,j)}^{(k,l)} \cdot x_{(c,i+u-1,j+v-1)}^{(l-1)}
\end{aligned}
$$

在卷积神经网络的结构中，在卷积层之后的层有俩种类型：
- 从卷积层连接到卷积层的情况(从卷积层开始反向传播的情况)
- 从卷积层连接到全连接层的情况(从全连接层开始反向传播的情况)

池化层的德尔塔：没有通过池化的单元相应的德尔塔都应作为0来参与计算。    
通过和没通过的，要分开考虑。    

与全连接层相连的卷积层的德尔塔：

$$
\begin{aligned}
\delta_{(i,j)}^{(k,l)} &= \frac{\partial E(\Theta)}{\partial z_{(i,j)}^{(k,l)}} \\
  &= \sum_{r=1}^{m^{(l+1)}}\delta_{r}^{(l+1)} \cdot w_{(r,k,i,j)}^{(l+1)} a \prime ^{(l)}\big(z_{(i,j)}^{(k,l)}\big) \\
  &= a \prime ^{(l)}\big(z_{(i,j)}^{(k,l)}\big) \sum_{r=1}^{m^{(l+1)}}\delta_{r}^{(l+1)} w_{(r,k,i,j)}^{(l+1)}
\end{aligned}
$$


- k: 第k个过滤器      
- l: 第l层    
- i,j: 位置    
- u,v,c: 纵、横、通道    

- $K^{(l+1)}$: 第l+1层卷积过滤器的个数          
- $m^{(l+1)} x m^{(l+1)}$: 大小
- (i,j): 特征图的位置
- $(p_i,p_j)$: 对应的池化层位置

与卷积层相连的卷积层的德尔塔：

$$
\begin{aligned}
\delta_{(i,j)}^{(k,j)} &= \sum_{q=1}^{K^{(l+1)}}\sum_{r=1}^{m^{(l+1)}}\sum_{s=1}^{m^{(l+1)}}\delta_{(p_i-r+1,p_j-s+1)}^{(q,l+1)}\cdot w_{(k,r,s)}^{q,l+1}\cdot a \prime ^{(l)}\big(p_{(k,l)}^{(i,j)} \big) \\
    &= a \prime ^{(l)}\big(p_{(k,l)}^{(i,j)} \big) \sum_{q=1}^{K^{(l+1)}}\sum_{r=1}^{m^{(l+1)}}\sum_{s=1}^{m^{(l+1)}}\delta_{(p_i-r+1,p_j-s+1)}^{(q,l+1)}w_{(k,r,s)}^{q,l+1}
\end{aligned}
$$

综上参数的更新：

$$
\begin{aligned}
w_{ij}^{(l)} &:= w_{ij}^{(l)} - \eta\delta_{i}^{(l)}x_{j}^{(l-1)} \quad \cdots\cdots \text{全连接层的权重} \\
b_{(l)} &:= b_{(l)} - \eta\delta_{i}^{(l)} \quad \cdots\cdots \text{全连接层的偏置} \\
w_{(c,u,v)}^{(k,l)} &:= w_{(c,u,v)}^{(k,l)} - \eta\sum_{i=1}^{d}\sum_{j=1}{d}\delta_{(i,j)}^{(k,l)}x_{(c,i+u-1,j+v-1)}^{(l-1)}  \quad \cdots\cdots \text{卷积过滤器的权重} \\
b^{(k,l)} &:= b^{(k,l)} - \eta\sum_{i=1}{d}\sum_{j=1}{d}\delta_{(i,j)}^{(k,l)} \quad \cdots\cdots \text{卷积过滤器的偏置} 
\end{aligned}
$$

交叉熵的数学表达式：

$$
\begin{aligned}
H(P,Q) = - \sum_{w \in \Omega}P(w)log_2Q(w)
\end{aligned}
$$

# 第 5 章 实现神经网络
1. 使用python实现3层神经网络
- 输入层
- 隐藏层 1
- 隐藏层 2
- 输出层 3

```Python
import numpy as np
import math

# 训练数据的数量
N = 1000

# 固定种子的值
np.random.seed(1)

# 随机生成训练数据和正确答案的标签
TX = (np.random.rand(N, 2) * 1000).astype(np.int32) + 1
TY = (TX.min(axis=1) / TX.max(axis=1) <= 0.2).astype(np.int32)[np.newaxis].T
print(TX[0])

# 计算平均值和标准差
MU = TX.mean(axis=0)
SIGMA = TX.std(axis=0)


# 标准化
def standardize(X):
    return (X - MU) / SIGMA


TX = standardize(TX)

# 神经网络的结构
# 权重和偏置
W1 = np.random.randn(2, 2)  # 第1层的权重
W2 = np.random.randn(2, 2)  # 第2层的权重
W3 = np.random.randn(1, 2)  # 第3层的权重

b1 = np.random.randn(2)  # 第1层的偏置
b2 = np.random.randn(2)  # 第2层的偏置
b3 = np.random.randn(1)  # 第3层的偏置


# 定义激活函数 sigmoid函数
def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))


# 正向传播
# def forward(x0):
#     z1 = np.dot(W1, x0) + b1
#     x1 = sigmoid(z1)
#     z2 = np.dot(W2, x1) + b2
#     x2 = sigmoid(z2)
#     z3 = np.dot(W3, x2) + b3
#     x3 = sigmoid(z3)
#     return z1,x1,z2,x2,z3,x3
#
def forward(X0):
    Z1 = np.dot(X0, W1.T) + b1
    X1 = sigmoid(Z1)
    Z2 = np.dot(X1, W2.T) + b2
    X2 = sigmoid(Z2)
    Z3 = np.dot(X2, W3.T) + b3
    X3 = sigmoid(Z3)
    return Z1, X1, Z2, X2, Z3, X3


# 反向传播
# sigmoid函数的微分
def dsigmoid(x):
    return (1.0 - sigmoid(x)) * sigmoid(x)


# 输出层的德尔塔
def delta_output(Z, Y):
    return (sigmoid(Z) - Y) * dsigmoid(Z)


# 隐藏层的德尔塔
def delta_hidden(Z, D, W):
    return dsigmoid(Z) * np.dot(D, W)


def backward(Y, Z3, Z2, Z1):
    D3 = delta_output(Z3, Y)
    D2 = delta_hidden(Z2, D3, W3)
    D1 = delta_hidden(Z1, D2, W2)
    return D3, D2, D1


# 训练
ETA = 0.001


# 对目标函数的权重进行微分
def dweight(D, X):
    return np.dot(D.T, X)


# 对目标函数的偏置进行微分
def dbias(D):
    return D.sum(axis=0)


# 更新参数
def update_parameters(D3, X2, D2, X1, D1, X0):
    global W3, W2, W1, b3, b2, b1
    W3 = W3 - ETA * dweight(D3, X2)
    W2 = W2 - ETA * dweight(D2, X1)
    W1 = W1 - ETA * dweight(D1, X0)
    b3 = b3 - ETA * dbias(D3)
    b2 = b2 - ETA * dbias(D2)
    b1 = b1 - ETA * dbias(D1)


# 训练函数
def train(X, Y):
    # 正向传播
    Z1, X1, Z2, X2, Z3, X3 = forward(X)
    # 方向传播
    D3, D2, D1 = backward(Y, Z3, Z2, Z1)
    update_parameters(D3, X2, D2, X1, D1, X)


# 重复次数
EPOCH = 30000


# 预测
def predict(X):
    return forward(X)[-1]


# 目标函数
def E(Y, X):
    return 0.5 * ((Y - predict(X)) ** 2).sum()

# 小批量的大小
BATCH = 100

for epoch in range(1,EPOCH+1):
    # 获得用于小批量训练的随机索引
    p = np.random.permutation(len(TX))
    # 取出数量为小批量大小的数据并训练
    for i in range(math.ceil(len(TX) / BATCH)):
        indice = p[i*BATCH:(i+1)*BATCH]
        X0 = TX[indice]
        Y = TY[indice]
        train(X0,Y)
    # 输出日志
    if epoch % 1000 == 0:
        log = '误差 = {:8.4f} (第 {:5d} 轮)'
        print(log.format(E(TY, TX),epoch))

# 测试
testX = standardize([
    [100,100],
    [100,10],
    [10,100],
    [80,100],
])

print(predict(testX))

# 分类器
def classify(X):
    return (predict(X)>0.8).astype(np.int32)

print(classify(testX))

# 生成测试数据
TEST_N = 1000
testX = (np.random.rand(TEST_N, 2) * 1000).astype(np.int32) + 1
testY = (testX.min(axis=1) / testX.max(axis=1) <= 0.2).astype(np.int32)[np.newaxis].T

accuracy = (classify(standardize(testX)) == testY).sum() / TEST_N
print("精度 : {}%".format(accuracy * 100))
```
