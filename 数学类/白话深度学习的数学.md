# 第1章 神经网络入门
## 1.2 神经网络所处的位置
AI:
- 基于规则
- 机器学习
  - 朴素贝叶斯
  - SVM
  - 线性回归
  - 感知机
  - 逻辑回归
  - 神经网络
    - DNN
    - CNN
    - RNN
    - GAN

## 1.3 关于神经网络

输入值 -> 输入层 -> 隐藏层 -> 输出层 -> 输出值

圆形表示**神经元**,又被称为**单元**      
单元之间的每个连接都带有叫作**权重**的数值，这些数值是衡量信息的重要性或相关性的指标    

增加层次，网络越来越深而形成的神经网络，叫作**深度神经网络**，又称**DNN**    
学习这种深层的神经网络的权重，叫作**深度学习**，或**深层学习**    
单元之间的连接方式也可以改变。**卷积神经网络**，就是通过更改单元的连接方式而形成的。    

# 1.4 神经网络能做的事情
神经网络实际上就是一个函数 $f(x)$     

回归问题    
分类问题    
生成性的任务    

# 第2章 学习正向传播
## 2.2 感知机的原理

$$
y =
\begin{cases}
0, & (w_{1}x_{1} + w_{2}x_{2} \leq \theta)  \\
1, & (w_{1}x_{1} + w_{2}x_{2} > \theta)
\end{cases}
$$

$$
y=
\begin{cases}
0, & \bigg(\displaystyle\sum_{i=1}^{2} w_{i}x_{i} \leq \theta\bigg) \\
1, & \bigg(\displaystyle\sum_{i=1}^{2} w_{i}x_{i} > \theta\bigg)
\end{cases}
$$


也可以用向量来表示

$$
x = 
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
$$

$$
w = 
\begin{bmatrix}
w_1 \\
w_2
\end{bmatrix}
$$

$$
y = 
\begin{cases}
0, & (w \cdot x \leq \theta)\\
1, & (w \cdot x > \theta)
\end{cases}
$$

## 2.3 感知机和偏置

$$
y = 
\begin{cases}
0, & (w \cdot x + b \leq 0)\\
1, & (w \cdot x + b > 0)
\end{cases}
$$

其中用b表示 $-\theta$ , b叫作**偏置**      

## 2.7 多层感知机

**多层感知机就是神经网络**

**多层构成，且每层的所有单元都由箭头连接起来的网络，叫全连接神经网络**

## 2.9 神经网络的权重

- 权重的数量 = 连接各层中单元的线的数量
- 偏置的数量 = 该层的单元的数量

## 2.10 激活函数

激活函数的核心价值：打破线性限制、提供非线性映射能力、稳定数值范围、优化梯度传播。

没有激活函数，深度网络无法体现深度的优势。

根据阈值输出0或1的函数，叫作阶跃函数。

| 激活函数       | 公式                               | 特点                            |
| ---------- | -------------------------------- | ----------------------------- |
| Sigmoid    | $\sigma(x) = \frac{1}{1+e^{-x}}$ | 输出(0,1)，概率解释，梯度消失问题           |
| Tanh       | $\tanh(x)$                       | 输出(-1,1)，收敛快于Sigmoid，但仍可能梯度消失 |
| ReLU       | $\max(0,x)$                      | 简单高效，稀疏激活，避免梯度消失，但存在“神经元死亡”   |
| Leaky ReLU | $x>0:x,\;x<0:ax$                 | 改进ReLU，缓解神经元死亡问题              |
| Softmax    | $\frac{e^{x_i}}{\sum e^{x_j}}$   | 多分类输出概率分布                     |

## 2.11 神经网络的表达式
两层神经网络的函数表达式：输入层->隐藏层->输出层

$$
f(x^{(0)}) = a^{(2)} \big(W^{(2)}a^{(1)}(W^{(1)}x^{(0)} + b^{(1)})+ b^{(2)})
$$

其中 $a^{(i)}$ 表示对应层的激活函数， $W^{(i)}$ 表示对应层的权重矩阵， $b^{(i)}$ 表示对应层的偏置矩阵 

## 2.12 正向传播
两层神经网络的正向传播：  

$$
\begin{aligned}
x^{(1)} &= a^{(1)}(W^{(1)}x^{(0)} + b^{(1)}) \\         
x^{(2)} &= a^{(2)}(W^{(2)}x^{(1)} + b^{(2)}) \\    
f(x^{(0)}) &= x^{(2)}
\end{aligned}
$$

从左到右一层层传递的操作叫作正向传播

## 2.13 神经网络的通用化

L 层神经网络正向传播的公式：

$$
\begin{aligned}
x^{(1)} &= a^{(1)}(W^{(1)}x^{(0)} + b^{(1)}) \\
x^{(2)} &= a^{(2)}(W^{(2)}x^{(1)} + b^{(2)}) \\
\vdots \\
y &= a^{(L)}(W^{(L)}x^{(L-1)}+b^{(L)})
\end{aligned}
$$

# 第3章 学习反向传播

## 3.4 目标函数
误差公式

$$
\begin{aligned}
E(\Theta) = \frac{1}{2} \sum_{k=1}^{n} (y_k - f(x_k))^2
\end{aligned}
$$

乘以正数，只会是误差之和上下变化，而使误差之和最小的 $\Theta$ 本身的值不会发生变化。    

这种寻找某个函数值最小的参数的问题叫作**最优化问题**    
而在最优化问题中，寻找最小值的函数叫作**目标函数**， $E(\Theta)$ 就是目标函数    

## 3.5 梯度下降法

有表达式为 $g(x) = (x - 1)^2$的函数，求g(x)最小的x。    

微分求解

$$
\begin{aligned}
\frac{dg(x)}{dx} &= \frac{d}{dx}(x-1)^2 \\
    &= \frac{d}{dx}(x^2-2x+1) \\
    &= 2x-2
\end{aligned}
$$

- 当 x < 1 时，增大 x , g(x) 减小
- 当 x > 1 时，减小 x , g(x) 减小

根据导数的符号，创建增减表

|x的范围|$\frac{dg(x)}{dx}$的符号|g(x)的增减|要使g(x)最小，该如何做|
|:--:|:--:|:--:|:--:|
|$x < 1$|-|↘️|增大x|
|$x = 1$|0||已是最小值|
|$x > 1$|+|↗️|减小x|

沿着与导函数的符号相反的方向移动，可直接写成： $x: = x- \frac{dg(x)}{dx}$    

**A:=B** 的意思是 A 是由B定义的。

当x的移动幅度过大时，会导致无法找到最小值，因此需要增加一个参数伊塔，叫做**学习率**，记作 $\eta$ ,此时公式为： 

$$
\begin{aligned}
x := x - \eta\frac{dg(x)}{dx}
\end{aligned}
$$

如果 $\eta$ 太大，会导致x跳来跳去，甚至可能远离最小值，这种状态叫作**发散**。    
反之，如果取较小的 $\eta$ 值，会使x的移动量变小。虽然能够使它接近最小值，但也会增加更新的次数。这种状态叫**收敛**。    

使用微分来解决最优化问题的方法，这种方法叫作**梯度下降法**


更新目标函数的公式：

$$
\begin{aligned}
\Theta := \Theta - \eta\frac{d}{d \Theta}E(\Theta)
\end{aligned}
$$

$\Theta = {W^{(1)},b^{(1)},W^{(2)},b^{(2)}}$

更新参数的**偏微分**：    

$$
\begin{aligned}
w_{ij}^{(l)} := w_{ij}^{(l)} - \eta\frac{\partial E(\Theta)}{\partial w_{ij}^{(l)}}
\end{aligned}
$$

$$
\begin{aligned}
b_{i}^{(l)} := b_{i}^{(l)} - \eta\frac{\partial E(\Theta)}{\partial b_{i}^{(l)}}
\end{aligned}
$$

## 3.6 小技巧：德尔塔
误差之和公式：

$$
\begin{aligned}
E(\Theta) = \frac{1}{2}\sum_{k=1}^{n}(y_{k} - f(x_k))^2
\end{aligned}
$$

拆解开，一项项的求，如：

$$
\begin{aligned}
E_{k}(\Theta) = \frac{1}{2}(y_k - f(x_k))^2
\end{aligned}
$$

总结就是：先求误差之和，再计算整体的偏微分，和先计算各个误差的偏微分，再求和，结果是一样的。即如下式：

$$
\begin{aligned}
\frac{\partial}{\partial w_{ij}^{(l)}}\big(\sum_{k=1}^{n}E_{k}(\Theta)\big) = \sum_{k=1}^{n}\big(\frac{\partial E_{k}(\Theta)}{\Theta w_{11}^{(l)}}\big)
\end{aligned}
$$


例如在处理有k个数据的 $x_k$ 时，神经网络层与层之间传递数值计算过程如下所示：

$$
\begin{aligned}
&x^{(0)} = x_k    \cdots\ \text{输出层} \\
&x^{(1)} = a^{(1)} \big(W^{(1)}x^{(0)} + b^{(1)} \big)    \cdots\ \text{第 1 层} \\
&x^{(2)} = a^{(2)} \big(W^{(2)}x^{(1)} + b^{(2)} \big)    \cdots\ \text{第 2 层} \\
&x^{(3)} = a^{(3)} \big(W^{(3)}x^{(2)} + b^{(3)} \big)    \cdots\ \text{第 3 层} \\
&f(x_k) = x^{(3)}
\end{aligned}
$$

反过来从输出值的角度来看:    

$$\displaystyle
\begin{aligned}
f(x_k) &= x^{(3)} \quad \cdots\ \text{输出层} \\
    &= a^{(3)} \big(W^{(3)}x^{(2)} + b^{(3)} \big) \quad \cdots\ \text{第 3 层} \\
    &= a^{(3)} \Bigg(
        \begin{bmatrix}
        w_{11}^{(3)} & w_{12}^{(3)}
        \end{bmatrix}
        \begin{bmatrix}
        x_{1}^{(2)} \\
        x_{2}^{(2)}
        \end{bmatrix}
        +
        \begin{bmatrix}
        b_{1}^{(3)}
        \end{bmatrix}
        \Bigg) \\
    &= a^{(3)}(w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)}) \quad \cdots\ \text{展开权重和偏置}
\end{aligned}
$$

令 $z_{1}^{(3)} = w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)}$ ,可以把z1看作是应用激活函数之前的第3层的第1个单元的输入，也叫**加权输入**     

那么误差可以写成如下公式：

$$
\begin{aligned}
E_{k}(\Theta) &= \frac{1}{2}(y_k - f(x_k))^2 \\
    &= \frac{1}{2}(y_k - a^{(3)}z_{1}^{(3)})^2 
\end{aligned}
$$

那么微分方程就为：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{11}^{(3)}} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \cdot \frac{\partial z_{1}^{(3)}}{\partial w_{11}^{(3)}}
\end{aligned}
$$

那么 $z_{1}^{(3)}$ 对 $w_{11}^{(3)}$ 的偏微分，如下式：

$$
\begin{aligned}
\frac{\partial z_{1}^{(3)}}{\partial w_{11}^{(3)}} &= \frac{\partial}{\partial w_{11}^{(3)}} \big( w_{11}^{(3)}x_{1}^{(2)} + w_{12}^{(3)}x_{2}^{(2)} + b_{1}^{(3)} \big) \\
    &= x_{1}^{(2)}
\end{aligned}
$$

接下来，用德尔塔来代替，如下所示：

$$
\delta_{1}^{(3)} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}}
$$

$\delta$ **常用来表示微小的变化量**。

那么

$$
\begin{aligned}
& \frac{\partial E_{k}(\Theta)}{\partial w_{11}^{(3)}} = \delta_{1}^{(3)} \cdot x_{1}^{(2)} \\
& \frac{\partial E_{k}(\Theta)}{\partial w_{12}^{(3)}} = \delta_{1}^{(3)} \cdot x_{2}^{(2)} 
\end{aligned}
$$

推广成一般，表达式变为如下格式：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}} = \delta_{i}^{(l)} \cdot x_{j}^{(l)} \quad \big( \delta_{i}^{(l)} = \frac{\partial E_{k}(\Theta)}{\partial z_{i}^{(l)}} \big)
\end{aligned}
$$

**理念：德尔塔的复用**

## 3.7 德尔塔的计算
1. 输出层的计算

$$
\begin{aligned}
\text{设} v &= y_k - a^{(3)} \cdot z_{1}^{(3)} \\
E_{k}(\Theta) &= \frac{1}{2}v^2
\end{aligned}
$$

$z_{1}^{(3)}$ 包含在 v 中，v 包含在 $E_{k}(\Theta)$    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} &= \frac{\partial{E_{k}(\Theta)}}{\partial v} \cdot \frac{\partial v}{\partial z_{1}^{(3)}} \\
\frac{\partial{E_{k}(\Theta)}}{\partial v} &= \frac{\partial}{\partial v}\big(\frac{1}{2}v^2 \big) \\
    &= v \\
\frac{\partial v}{\partial z_{1}^{(3)}} &= \frac{\partial}{\partial z_{1}^{(3)}}\big(y_k - a^{(3)} \cdot z_{1}^{(3)} \big) \\
    &= -a \prime ^{3}(z_{1}^{3}) \quad \cdots  -a \prime ^{3}(z_{1}^{3}) == \frac{\partial a^{(3)}(z_{1}^{(3)})}{\partial z_{1}^{(3)}}
\end{aligned}
$$

综上：

$$
\begin{aligned}
\delta_{1}^{(3)} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \\
    &= \frac{\partial E_{k}(\Theta)}{\partial v} \cdot \frac{\partial v}{\partial z_{1}^{(3)}} \\
    &= v \cdot a \prime ^{(3)}(z_{1}^{(3)}) \\
    &= (y_k - a^{(3)}(z_{1}^{(3)})) \cdot - a \prime ^{(3)}(z_{1}^{(3)}) \\
    &= (a^{(3)}(z_{1}^{(3)}) - y_k) \cdot + a \prime ^{(3)}(z_{1}^{(3)})
\end{aligned}
$$

如果激活函数是Sigmoid函数，则微分可表示成：

$$
\begin{aligned}
a \prime ^{(3)}(z_{1}^{(3)}) = (1 - a^{(3)}(z_{1}^{(3)})) \cdot a^{(3)}(z_{1}^{(3)})
\end{aligned}
$$

2. 隐藏层的计算

$$
\begin{aligned}
& \delta_{1}^{(2)} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(2)}} ,  \delta_{2}^{(2)} = \frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(2)}} \quad \cdots \cdots \text{第 2 层的德尔塔} \\
& \delta_{1}^{(1)} = \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} ,  \delta_{2}^{(1)} = \frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(1)}} \quad \cdots \cdots \text{第 1 层的德尔塔}
\end{aligned}
$$


- 输入层(0层):  $x_1$ , $x_2$
- 隐藏层(1层):  $z_{1}^{(1)} \quad \delta_{1}^{(1)}$ , $z_{2}^{(1)} \quad \delta_{2}^{(1)}$
- 隐藏层(2层):  $z_{1}^{(2)} \quad \delta_{1}^{(2)}$ , $z_{2}^{(2)} \quad \delta_{2}^{(2)}$
- 隐藏层(3层):  $z_{1}^{(3)} \quad \delta_{1}^{(3)}$
- 输出层: y

$z_{1}^{(2)}$ 在 $z_{1}^{(3)}$ 中， $z_{1}^{(3)}$ 在 $E_{k}(\Theta)$ 中    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}(2)} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \cdot \frac{\partial z_{1}^{(3)}}{\partial z_{1}^{(2)}} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}(2)} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(3)}} \cdot \frac{\partial z_{1}^{(3)}}{\partial z_{2}^{(2)}}
\end{aligned}
$$

同理 $z_{1}^{(1)}$ 包含在 $z_{1}^{(2)}$ 和 $z_{2}^{(2)}$ 中， 而 $z_{1}^{(2)}$ 和 $z_{2}^{(2)}$ 包含在 $E_{k}(\Theta)$ 中    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} &= \frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(2)}} \cdot \frac{\partial z_{1}^{(2)}}{\partial z_{1}^{(1)}} + 
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(2)}} \cdot \frac{\partial z_{2}^{(2)}}{\partial z_{1}^{(1)}}
\end{aligned}
$$

求和符号整理成：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{1}^{(1)}} \big) \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{2}^{(1)}} \big)
\end{aligned}
$$

综上各层的公式如下：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(2)}} &= \sum_{r=1}^{1} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(3)}} \cdot \frac{\partial z_{r}^{(3)}}{\partial z_{1}^{(2)}} \big) \quad \cdots \cdots \text{第 2 层第 1 个德尔塔} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(2)}} &= \sum_{r=1}^{1} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(3)}} \cdot \frac{\partial z_{r}^{(3)}}{\partial z_{2}^{(2)}} \big) \quad \cdots \cdots \text{第 2 层第 2 个德尔塔} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{1}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{1}^{(1)}} \big) \quad \cdots \cdots \text{第 1 层第 1 个德尔塔} \\
\frac{\partial E_{k}(\Theta)}{\partial z_{2}^{(1)}} &= \sum_{r=1}^{2} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(2)}} \cdot \frac{\partial z_{r}^{(2)}}{\partial z_{2}^{(1)}} \big) \quad \cdots \cdots \text{第 1 层第 2 个德尔塔}
\end{aligned}
$$

总结：

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{i}^{(l)}} = \sum_{r=1}^{m^{(l+1)}} \big( \frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(l+1)}} \cdot \frac{\partial z_{r}^{(l+1)}}{\partial z_{i}^{(l)}} \big)
\end{aligned}
$$

而 $z_{r}^{(l+1)}$ 展开

$$
\begin{aligned}
z_{r}^{(l+1)} &= w_{r1}^{(l+1)}x_{1}^{(l)} + \cdots + w_{ri}^{(l+1)}x_{i}^{(l)} + \cdots \\
    &= w_{r1}^{(l+1)}a^{(l)}(z_{1}^{(l)}) + \cdots + w_{ri}^{(l+1)}a^{(l)}(z_{i}^{(l)}) + \cdots
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial z_{r}^{(l+1)}}{\partial z_{i}^{(l)}} = w_{ri}^{(l+1)} a \prime ^{(l)}(z_{i}^{(l)})
\end{aligned}
$$

目标函数 $E_{k}(\Theta)$ 对加权输入的偏微分，正式德尔塔    

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial z_{r}^{(l+1)}} = \delta_{r}^{(l+1)}
\end{aligned}
$$

综上隐藏层的德尔塔的表达式如下：

$$
\begin{aligned}
\delta_{i}^{(l)} = \sum_{r=1}^{m^{(l+1)}} \big( \delta_{r}^{(l+1)} \cdot w_{ri}^{(l+1)} a \prime ^{(l)}(z_{i}^{(l)}) \big)
\end{aligned}
$$

## 3.8 方向传播

$$
\begin{aligned}
\delta_{i}^{(L)} &= \big( a^{(L)}(z_{i}^{(L)}) - y_k \big) \cdot a \prime ^{(L)}(z_{i}^{(L)})  \quad \cdots\cdots \text{输出层的德尔塔} \\
\delta_{i}^{(l)} &= \sum_{r=1}^{m^{(l+1)}} \big( \delta_{r}^{(l+1)} \cdot w_{ri}^{(l+1)} a \prime ^{(l)}(z_{i}^{(l)}) \big) \quad \cdots\cdots \text{隐藏层的德尔塔}
\end{aligned}
$$

从后面依次计算德尔塔，就能复用已经计算好的德尔塔
- 首先求第3层(输出层)的德尔塔
- 求第2层的德尔塔时，复用紧随其后的第3层的德尔塔
- 求第1层的德尔塔时，复用紧随其后的第2层的德尔塔

为了使用这个更新表达式更新权重，需要计算目标函数 $E_{k}(\Theta)$ 对权重的偏微分 

$$
\begin{aligned}
w_{ij}^{(l)} := w_{ij}^{(l)} - \eta \frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}}
\end{aligned}
$$

直接计算偏微分非常麻烦，所以使用德尔塔来间接地计算偏微分

$$
\begin{aligned}
\frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}} = \delta_{i}^{(l)} - x_{j}^{(l-1)}
\end{aligned}
$$

将所有表达式整合

$$
\begin{aligned}
\delta_{i}^{(L)} &= \big( a^{(L)}(z_{i}^{(L)}) - y_k \big) a \prime ^{(L)}(z_{i}^{(L)}) \quad \cdots\cdots \text{输出层的德尔塔}  \\ 
\delta_{i}^{(l)} &= a \prime ^{(l)}(z_{i}^{(l)}) \sum_{r=1}^{m^{(l+1)}} \delta_{r}^{(l+1)} w_{ri}^{(l+1)} \quad \cdots\cdots \test{隐藏层的德尔塔} \\
w_{ij}^{(l)} := w_{ij}^{(l)} - \eta \frac{\partial E_{k}(\Theta)}{\partial w_{ij}^{(l)}} \quad \cdots\cdots \test{权重的更新表达式} \\
\end{aligned}
$$

从后面的层开始计算德尔塔，以更新权重和偏置的方法叫作误差反向传播法，backpropagation ,又称 backward
